---
title: "Objective 1"
author: "Satvik Ajmera"
output: html_notebook
editor_options: 
  chunk_output_type: console
---
```{r}
df <- read.csv("https://raw.githubusercontent.com/JosephLazarus/Life_Expectancy/main/Data_Folder/clean_scaled.csv", header = TRUE, fileEncoding="UTF-8-BOM")

setwd("~/Desktop/SMU MSDS Course Material/Summer 2021/DS 6372/Project1_Summer_2021/GitHub Repo/Life_Expectancy")

df2 <- read.csv("WHO_Data/Life Expectancy Data.csv", header = TRUE)
df2
```


```{r}
library(caret)
library(naniar)
sapply(df, function(x) sum(is.na(x)))
gg_miss_var(df)
sapply(df2, function(x) sum(is.na(x)))
gg_miss_var(df2)

sum(is.na(df))
sum(is.na(df2))
df$Developed = as.factor(df$Developed)
#removed log.under.five.deaths, because of high colinearity
removed_df <- subset(df, select = -c(log.under.five.deaths))
#omit NA values for train/test split
clean.re.df <-na.omit(removed_df)
#omit NA values for train/test split
clean.df <- na.omit(df)
df$Developed = as.factor(df$Developed)
clean.df2 <- na.omit(df2)
fit = lm(Life.expectancy~., data = df)
plot(fit)

#We have remove NA values before creating data partition, so we used clean.df
set.seed(1234)
trainIndex <- createDataPartition(clean.df$Life.expectancy,p=.8,list=F)
training <- clean.df[trainIndex,]
validate <- clean.df[-trainIndex,]

#Check our train/test split
dim(training)
dim(validate)
```

Forward Selection Using Leaps Package
```{r}
library(leaps)
#Too many linear dependencies found
regfit.full = regsubsets(Life.expectancy~.,data=clean.df, nvmax=6)
reglife.bwd=regsubsets(Life.expectancy~.,data=clean.df,method="backward",nvmax=10)

```

Forward Selection using Caret Package
```{r}
fitControlForward<-trainControl(method="cv")
x=model.matrix(Life.expectancy~.,training)[,-1]
colnames(x)
View(x)
y=training$Life.expectancy
f.fit<-train(x=x,y=y, 
               method="leapForward",
               trControl=fitControlForward
               )

f.fit
```


```{r}
library(glmnet)
fitControl<-trainControl(method="repeatedcv",number=10,repeats=10)
#To get model names for `method`
names(getModelInfo())
#GLM Net Model (selecting tuning parameters alpha and lambda via 10 FOLD CV)
set.seed(1234)
glmnet.fit<-train(Life.expectancy~.,
               data=training,
               method="glmnet",
               trControl=fitControl,
               na.action = na.omit
               )
#glmnet.fit results
glmnet.fit
#Model Coefficients
coef(glmnet.fit$finalModel,glmnet.fit$finalModel$lambdaOpt)
```

```{r}
#Creating using the validation set. Resulting in the RMSE of the validation set
glmnet.pred<-predict(glmnet.fit,validate)
RMSE(glmnet.pred, validate$Life.expectancy)
#RMSE
glmnet.RMSE<-sqrt(mean((validate$Life.expectancy-glmnet.pred)^2))
glmnet.RMSE
plot(glmnet.pred,validate$Life.expectancy,ylim=c(40,100),xlim=c(40,100))
lines(0:100,0:100)


#Here is a more natural tool to compute RMSE as well as some additional metrics
glmnet.validate<-postResample(pred = glmnet.pred, obs = validate$Life.expectancy)
glmnet.validate

#Ranking of the predictors
varImp(glmnet.fit)
plot(varImp(glmnet.fit))
```

Lasso with glmnet Package
```{r}
library(glmnet)
x <- model.matrix(Life.expectancy~.,training)[,-1]
y <- training$Life.expectancy
xtest <- model.matrix(Life.expectancy~.,validate)[,-1]
ytest <- validate$Life.expectancy


grid=10^seq(10,-2, length =100)
lasso.mod=glmnet(x,y,alpha=1, lambda=grid)

cv.out=cv.glmnet(x,y,alpha=1) #alpha=1 performs LASSO
plot(cv.out)

bestlambda <- cv.out$lambda.min
bestlambda
lasso.pred=predict(lasso.mod ,s=bestlambda ,newx=xtest)
testMSE_LASSO<-mean((ytest-lasso.pred)^2)
testMSE_LASSO

coef(lasso.mod,s=bestlambda)
```


```{r}

```

