---
title: 'Objective 2: Experimental Models'
author: "Joseph Lazarus"
date: "5/28/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

rated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r life expectancy data}
library(ggplot2)

#data is cleaned and scaled (Friday May 31th)
df <- read.csv("https://raw.githubusercontent.com/JosephLazarus/Life_Expectancy/main/Data_Folder/clean_scaled.csv", header = TRUE, fileEncoding="UTF-8-BOM")


#removing NA's causing problems with dates. too many fields still missing
#which(is.na(df))

#still finding NAS remove them
df <- na.omit(df)


```

```{r}
library(ggthemes)
theme_set(theme_fivethirtyeight())
theme_update(axis.title = element_text()) #the default for fivethirtyeight is to not show axis labels, this removes that default so we can choose to specify and display axis titles
theme_update(plot.title = element_text(hjust = 0.5))
```




~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
                              Train/Test Split code
IMPORTANT:  I have not done a split on date. Date is not present in this cleaned_scaled.csv
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

```{r random split}

set.seed(31) # prime number for good luck

sample_size = round(nrow(df)*.70) # setting what is 70%
index <- sample(seq_len(nrow(df)), size = sample_size)

training.data <- df[index, ]
testing.data <- df[-index, ]

#sanity check the split
dim(df)
dim(training.data)
dim(testing.data)


```
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                                 Model Metrics
                                    AIC BIC
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


```{r AIC BIC functions}
AIC <- function(y, y_pred, n, k){
  resids = y - y_pred
  sse = sum(resids^2)
  AIC =  n * log(sse/n) + 2*(k + 1)
  print(return(AIC))
}

BIC <- function(y, y_pred, n, k){
  resids = y - y_pred
  sse = sum(resids^2)
  BIC = n * log(sse/n) + log(n) * (k+1)
  print(return(BIC))
}

```



~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                         train/test split on Date
                    
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

```{r Data split}
training.data <- df[c(df$Year <= 2010),]
testing.data <- df[c(df$Year > 2010),]

dim(df)
dim(training.data)
dim(testing.data)

training.data <- subset(training.data, select = -c(Country,Year))
testing.data <- subset(testing.data, select = -c(Country, Year))
```

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                             Step 3.1: Experimental Model Engine
                                       Knn Regression
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

```{r knn regression with caret}
library(caret)
library(plyr)
knn2 <- knnreg(training.data$Life.expectancy ~ ., data = training.data, k=2)

train.knn = function(mr.rogers){
  knnreg(training.data$Life.expectancy ~., data = training.data, k=mr.rogers)
} 

test.knn =  function(mr.rogers){
  knnreg(testing.data$Life.expectancy ~., data = testing.data, k=mr.rogers)
} 

k.to.try <- seq(1,50, by = 1)
#create and store list knn predictions for k = 1 thru fiddy
knn.list.train = lapply(k.to.try, train.knn)
knn.list.test = lapply(k.to.try, test.knn)


#create and store list of predictions
knn.train.predictions <- lapply(knn.list.train, predict, training.data)
knn.test.predictions <- lapply(knn.list.test, predict, testing.data)
```

Model Metrics RMSE
```{r Root Mean Square Error}
#create function for RMSE
calc.rmse <- function(actual, predicted){
  sqrt(mean(actual - predicted)^2)
}

#create and store matrix of RMSE
knn.rmse.train <- sapply(knn.train.predictions, calc.rmse, actual = training.data$Life.expectancy)
knn.rmse.test <- sapply(knn.test.predictions, calc.rmse, actual = testing.data$Life.expectancy)

#RMSE plot of training vs predicted 
plot(k.to.try, knn.rmse.test, type = "b", col = "dodgerblue", pch = 20, 
     ylim = range(c(knn.rmse.test, knn.rmse.train)),
     xlab = "K Neighbors",
    ylab = "RMSE Specificy units (years?",
    main = "Test and Training RMSE vs K")
lines(k.to.try, knn.rmse.train, type = "b", col = "darkorange", pch = 23)
index<-which(knn.rmse.test ==min(knn.rmse.test))
points(index,knn.ase.test[index],col="red",pch=10)
legend("topright", c("Train RMSE", "Test RMSE"), 
       col = c("darkorange", "dodgerblue"),
       lty = c(1), 
       pch = c(23, 20))
grid()
```

model Metrics ASE
```{r ASE}

#create function for MSE / ASE
calc.ase <- function(actual, predicted){
  (mean(actual - predicted)^2)
}

#create and store matrix of RMSE
knn.ase.train <- sapply(knn.train.predictions, calc.ase, actual = training.data$Life.expectancy)
knn.ase.test <- sapply(knn.test.predictions, calc.ase, actual = testing.data$Life.expectancy)

plot(k.to.try, knn.ase.test, type = "b", col = "dodgerblue", pch = 20, 
     ylim = range(c(knn.ase.test, knn.rmse.train)),
     xlab = "K Neighbors",
    ylab = "ASE",
    main = "Test and Training ASE vs K")
lines(k.to.try, knn.ase.train, type = "b", col = "darkorange", pch = 23)
index<-which(knn.ase.test ==min(knn.ase.test))
points(index,knn.ase.test[index],col="red",pch=10)
legend("topright", c("Train ASE", "Test ASE"), 
       col = c("darkorange", "dodgerblue"),
       lty = c(1), 
       pch = c(23, 20))
grid()



```

```{r f-ing ggplot}
#~~~~~~~~~~~~~~~~~~F you ggplot~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

#knnValues <- c(1:50)
#knn.rmse.train <-ldply(knn.train.predictions, calc.rmse, actual = training.data$Life.expectancy)
#knn.rmse.test <- ldply(knn.test.predictions, calc.rmse, actual = testing.data$Life.expectancy)

#df.rmse = as.data.frame(cbind(knnValues, knn.rmse.train , knn.rmse.test))

#colnames(df.rmse) <- c('k_value','RMSE_train','RMSE_test')

#ggplot(data = df.rmse, aes(x=k_value)) +
#  geom_line(aes(y=RMSE_train), color = "darkred") +
#  geom_line(aes(y=RMSE_test), color = "steelblue") +
#  labs(title = "RMSE of training vs Test Set" , x= "Value of K", y = 'RMSE') +
#  theme(legend.position = "botoom")
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



```

Model Diagnostics
```{r predicted vs expected}

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#removing country column from the data. 
#maybe set it to factor instead?

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


# lowest looks like knn = 38

# x = data frame of training set predictors
# y = numeric vector of outcomes
knnt3 <- knnreg(x= training.data[,-1], y=training.data[,1], k=1)

#predict(knnt3, newdata = testing.data[,-1])

#data frame of predicted vs expected
data.mod.knnt3 <- data.frame(predicted = predict(knnt3, newdata = testing.data[-1]), observed= testing.data[,1])

knnt3.mse <- round(mean((data.mod.knnt3$predicted - data.mod.knnt3$observed)^2), digits = 3)
knnt3.rmse <- round(sqrt(mean((data.mod.knnt3$predicted - data.mod.knnt3$observed)^2)), digits = 3)





table(knnt3.mse, knnt3.rmse)



```

```{r AIC & BIC knn}

AIC <- function(y, y_pred, n, k){
  resids = y - y_pred
  sse = sum(resids^2)
  AIC =  n * log(sse/n) + 2*(k + 1)
  print(return(AIC))
}

BIC <- function(y, y_pred, n, k){
  resids = y - y_pred
  sse = sum(resids^2)
  BIC = n * log(sse/n) + log(n) * (k+1)
  print(return(BIC))
}


AIC(data.mod.knnt3$observed, data.mod.knnt3$predicted, nrow(data.mod.knnt3), 21)

BIC(data.mod.knnt3$observed, data.mod.knnt3$predicted, nrow(data.mod.knnt3), 21)
```

```{r knn predicted vs expected}

#they said it couldn't be done but I did it. plotted observed vs predicted with caret package. 
ggplot(data.mod.knnt3,                                     
       aes(x = predicted,
           y = observed)) +
  geom_point() +
  geom_abline(intercept = 0,
              slope = 1,
              color = "red",
              size = 2)

```

```{r knn with selected variables}


```

```{r}

#find another knn regression model outside of caret
#install.packages("FNN", dependencies = TRUE, INSTALL_opts = '--no-lock')
library(FNN)
```

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                             Step 3.2: Experimental Model Engine
                                        Tree Models
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

```{r}
library(rpart)
library(rpart.plot)



dt.test.model <- rpart(training.data$Life.expectancy ~ ., data = training.data, method = 'anova', cp = .001)

rpart.plot(dt.test.model)

dt.model.pred = predict(dt.test.model, newdata = testing.data[,-1])
```

```{r tunning DT model}
#tunning the hyperparameter CP : default 0.01
# this means that the overall R-squared must increase by cp
# luckily plotcp does this for us!

plotcp(dt.test.model)
printcp(dt.test.model)

#i like round 17 cp = .0029175
# lets discuss why

dt.model <-  rpart(training.data$Life.expectancy ~ ., data = training.data, method = 'anova', cp = 0.007)

rpart.plot(dt.model)

dt.model.pred <- predict(dt.model, newdata = testing.data[,-1], cp = 0.007)

dt.model.ASE = mean((testing.data[,1] - predict(dt.model, testing.data[,-1]))^2)
dt.model.RMSE = sqrt(mean((testing.data[,1] - predict(dt.model, testing.data[,-1]))^2))

data.mod.dt <- data.frame(predicted = predict(dt.model), observed= training.data$Life.expectancy)

summary(dt.model)
AIC(dt.model)
# Draw plot using ggplot2 package this time
```

```{r AIC decesion tree}

AIC(data.mod.dt$observed, data.mod.dt$predicted, 19)

```

```{r DT model Expected Vs predicted}
ggplot(data.mod.dt,                                     
       aes(x = predicted,
           y = observed)) +
  geom_point() +
  geom_abline(intercept = 0,
              slope = 1,
              color = "red",
              size = 2)

```

```{r}

library(glmnet)



x = model.matrix(training.data$Life.expectancy ~ . , training.data)[,-1]
y = training.data$Life.expectancy

xtest<-model.matrix(testing.data$Life.expectancy~.,testing.data)[,-1]
ytest<-(testing.data$Life.expectancy)

grid=10^seq(10,-2, length =100)
lasso.mod=glmnet(x,y,alpha=1, lambda =grid)

cv.out=cv.glmnet(x,y,alpha=1) #alpha=1 performs LASSO
plot(cv.out)
bestlambda<-cv.out$lambda.min  #Optimal penalty parameter.  You can make this call visually.
lasso.pred=predict (lasso.mod ,s=bestlambda ,newx=xtest)

testMSE_LASSO<-mean((ytest-lasso.pred)^2)
testMSE_LASSO


coef(lasso.mod,s=bestlambda)

```