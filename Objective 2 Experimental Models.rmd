---
title: 'Objective 2: Experimental Models'
author: "Joseph Lazarus"
date: "5/28/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

rated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r life expectancy data}
library(ggplot2)

#data is cleaned and scaled (Friday May 31th)
df <- read.csv("https://raw.githubusercontent.com/JosephLazarus/Life_Expectancy/main/Data_Folder/clean_scaled.csv", header = TRUE, fileEncoding="UTF-8-BOM")


#removing NA's causing problems with dates. too many fields still missing
#which(is.na(df))

#still finding NAS remove them
df <- na.omit(df)


```

```{r}
library(ggthemes)
theme_set(theme_fivethirtyeight())
theme_update(axis.title = element_text()) #the default for fivethirtyeight is to not show axis labels, this removes that default so we can choose to specify and display axis titles
theme_update(plot.title = element_text(hjust = 0.5))
```




~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
                              Train/Test Split code
IMPORTANT:  I have not done a split on date. Date is not present in this cleaned_scaled.csv
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

```{r random split}

set.seed(31) # prime number for good luck

sample_size = round(nrow(df)*.70) # setting what is 70%
index <- sample(seq_len(nrow(df)), size = sample_size)

cst.training <- df[index, ]
cst.testing.data <- df[-index, ]

#sanity check the split
dim(df)
dim(cst.training)
dim(cst.testing.data)


```

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                         train/test split on Date
                    
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

```{r Data split}
cst.training <- df[c(df$Year <= 2010),]
cst.testing.data <- df[c(df$Year > 2010),]

dim(df)
dim(cst.training)
dim(cst.testing.data)

cst.training <- subset(cst.training, select = -c(Country,Year))
cst.testing.data <- subset(cst.testing.data, select = -c(Country, Year))
```

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                                 Model Metrics
                               AIC BIC RMSE MSE
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


```{r AIC BIC RMSE, MSE functions}
AIC <- function(y, y_pred, n, k){
  resids = y - y_pred
  sse = sum(resids^2)
  AIC =  n * log(sse/n) + 2*(k + 1)
  print(return(AIC))
}

BIC <- function(y, y_pred, n, k){
  resids = y - y_pred
  sse = sum(resids^2)
  BIC = n * log(sse/n) + log(n) * (k+1)
  print(return(BIC))
}

calc.rmse <- function(actual, predicted){
  sqrt(mean(actual - predicted)^2)
}

#create function for MSE / ASE
calc.ase <- function(actual, predicted){
  (mean(actual - predicted)^2)
}

```

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                             Step 3.1: Experimental Model Engine
                                       Knn Regression
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

```{r knn regression with caret}
library(caret)
library(plyr)
library(lattice)
knn2 <- knnreg(cst.training$Life.expectancy ~ ., data = cst.training, k=2)

cst.train.knn = function(mr.rogers){
  knnreg(cst.training$Life.expectancy ~., data = cst.training, k=mr.rogers)
} 

cst.test.knn =  function(mr.rogers){
  knnreg(cst.testing.data$Life.expectancy ~., data = cst.testing.data, k=mr.rogers)
} 

k.to.try <- seq(1,50, by = 1)
#create and store list knn predictions for k = 1 thru fiddy
cst.knn.list.train = lapply(k.to.try, cst.train.knn)
cst.knn.list.test = lapply(k.to.try, cst.test.knn)


#create and store list of predictions
cst.knn.train.predictions <- lapply(cst.knn.list.train, predict, cst.training)
cst.knn.test.predictions <- lapply(cst.knn.list.test, predict, cst.testing.data)
```

Model Metrics RMSE
```{r Root Mean Square Error}
#create function for RMSE


#create and store matrix of RMSE
cst.knn.rmse.train <- sapply(cst.knn.train.predictions, calc.rmse, actual = cst.training$Life.expectancy)
cst.knn.rmse.test <- sapply(cst.knn.test.predictions, calc.rmse, actual = cst.testing.data$Life.expectancy)

#RMSE plot of training vs predicted 
plot(k.to.try, cst.knn.rmse.test, type = "b", col = "dodgerblue", pch = 20, 
     ylim = range(c(cst.knn.rmse.test, cst.knn.rmse.train)),
     xlab = "K Neighbors",
    ylab = "RMSE Specificy units (years?",
    main = "Test and Training RMSE vs K")
lines(k.to.try, cst.knn.rmse.train, type = "b", col = "darkorange", pch = 23)
index<-which(cst.knn.rmse.test ==min(cst.knn.rmse.test))
points(index,cst.knn.ase.test[index],col="red",pch=10)
legend("topright", c("Train RMSE", "Test RMSE"), 
       col = c("darkorange", "dodgerblue"),
       lty = c(1), 
       pch = c(23, 20))
grid()
```

model Metrics ASE
```{r ASE}

#create and store matrix of RMSE
cst.knn.ase.train <- sapply(cst.knn.train.predictions, calc.ase, actual = cst.training$Life.expectancy)
cst.knn.ase.test <- sapply(cst.knn.test.predictions, calc.ase, actual = cst.testing.data$Life.expectancy)

plot(k.to.try, cst.knn.ase.test, type = "b", col = "dodgerblue", pch = 20, 
     ylim = range(c(cst.knn.ase.test, cst.knn.rmse.train)),
     xlab = "K Neighbors",
    ylab = "ASE",
    main = "Test and Training ASE vs K")
lines(k.to.try, cst.knn.ase.train, type = "b", col = "darkorange", pch = 23)
index<-which(cst.knn.ase.test ==min(cst.knn.ase.test))
points(index,cst.knn.ase.test[index],col="red",pch=10)
legend("topright", c("Train ASE", "Test ASE"), 
       col = c("darkorange", "dodgerblue"),
       lty = c(1), 
       pch = c(23, 20))
grid()



```

```{r f-ing ggplot}
#~~~~~~~~~~~~~~~~~~F you ggplot~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

#knnValues <- c(1:50)
#cst.knn.rmse.train <-ldply(cst.knn.train.predictions, calc.rmse, actual = cst.training$Life.expectancy)
#cst.knn.rmse.test <- ldply(cst.knn.test.predictions, calc.rmse, actual = cst.testing.data$Life.expectancy)

#df.rmse = as.data.frame(cbind(knnValues, cst.knn.rmse.train , cst.knn.rmse.test))

#colnames(df.rmse) <- c('k_value','RMSE_train','RMSE_test')

#ggplot(data = df.rmse, aes(x=k_value)) +
#  geom_line(aes(y=RMSE_train), color = "darkred") +
#  geom_line(aes(y=RMSE_test), color = "steelblue") +
#  labs(title = "RMSE of training vs Test Set" , x= "Value of K", y = 'RMSE') +
#  theme(legend.position = "botoom")
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



```

Model Diagnostics
```{r predicted vs expected}

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#removing country column from the data. 
#maybe set it to factor instead?

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


# lowest looks like knn = 38

# x = data frame of training set predictors
# y = numeric vector of outcomes
cst.knn.Model.AllVars.k38 <- knnreg(x= cst.training[,-1], y=cst.training[,1], k=38)
cst.knn.model.AllVars.k1 <- knnreg(x= cst.training[,-1], y=cst.training[,1], k=1)


#data frame of predicted vs expected k38
cst.knn.AllVars.OvP.df.k38 <- data.frame(predicted = predict(cst.knn.Model.AllVars.k38, newdata = cst.testing.data[-1]), observed= cst.testing.data[,1])

#dataframe of predicted cs expected k1
cst.knn.AllVars.OvP.df.k1 <- data.frame(predicted = predict(cst.knn.model.AllVars.k1, newdata = cst.testing.data[-1]), observed= cst.testing.data[,1])

cst.knn.AllVars.mse <- round(mean((cst.knn.AllVars.OvP.df.k38$predicted - cst.knn.AllVars.OvP.df.k38$observed)^2), digits = 3)
cst.knn.AllVars.rmse <- round(sqrt(mean((cst.knn.AllVars.OvP.df.k38$predicted - cst.knn.AllVars.OvP.df.k38$observed)^2)), digits = 3)





table(cst.knn.AllVars.mse, cst.knn.AllVars.rmse)



```

```{r AIC & BIC knn}

#  K=38 AIC
cst.knn.AllVars.k38.AIC <- AIC(cst.knn.AllVars.OvP.df.k38$observed, cst.knn.AllVars.OvP.df.k38$predicted, nrow(cst.knn.AllVars.OvP.df.k38), 21)
# K=38 BIC
cst.knn.AllVars.k38.BIC <- BIC(cst.knn.AllVars.OvP.df.k38$observed, cst.knn.AllVars.OvP.df.k38$predicted, nrow(cst.knn.AllVars.OvP.df.k38), 21)

# k =1 AIC
cst.knn.AllVars.k1.AIC <- AIC(cst.knn.AllVars.OvP.df.k1$observed, cst.knn.AllVars.OvP.df.k1$predicted, nrow(cst.knn.AllVars.OvP.df.k1), 21)
# k= 1 BIC
cst.knn.AllVars.k1.BIC <- BIC(cst.knn.AllVars.OvP.df.k1$observed, cst.knn.AllVars.OvP.df.k1$predicted, nrow(cst.knn.AllVars.OvP.df.k1), 21)

table(cst.knn.AllVars.k38.AIC, cst.knn.AllVars.k38.BIC, cst.knn.AllVars.k1.AIC,cst.knn.AllVars.k1.BIC)
```

```{r knn predicted vs observed}

# plotted observed vs predicted with caret package. 
ggplot(cst.knn.AllVars.OvP.df.k1,                                     
       aes(x = predicted,
           y = observed)) +
  geom_point() +
  geom_abline(intercept = 0,
              slope = 1,
              color = "red",
              size = 2) +
  labs(title = "Expected Vs Predicted K=1 All Vars")


# plot predicted vs observed K= 38 All Vars
ggplot(cst.knn.AllVars.OvP.df.k38,                                     
       aes(x = predicted,
           y = observed)) +
  geom_point() +
  geom_abline(intercept = 0,
              slope = 1,
              color = "red",
              size = 2) +
  labs(title = "Expected Vs Predicted K=38 All Vars")

```

```{r knn with selected variables}


```

```{r}

#find another knn regression model outside of caret
#install.packages("FNN", dependencies = TRUE, INSTALL_opts = '--no-lock')
library(FNN)
```

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                             Step 3.2: Experimental Model Engine
                                        Tree Models
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

```{r}
library(rpart)
library(rpart.plot)


#CP Default is 0.001
cst.dt.AllVars.test.model <- rpart(cst.training$Life.expectancy ~ ., data = cst.training, method = 'anova', cp = .001)

rpart.plot(cst.dt.AllVars.test.model)

cst.dt.AllVars.model.predictions = predict(cst.dt.AllVars.test.model, newdata = cst.testing.data[,-1])
```

```{r tunning DT model}
#tunning the hyperparameter CP : default 0.01
# this means that the overall R-squared must increase by cp
# luckily plotcp does this for us!

plotcp(cst.dt.AllVars.test.model)
printcp(cst.dt.AllVars.test.model)

#i like round 17 cp = .0029175
# lets discuss why

cst.dt.AllVars.model.cp0007 <-  rpart(cst.training$Life.expectancy ~ ., data = cst.training, method = 'anova', cp = 0.007)

rpart.plot(cst.dt.AllVars.model.cp0007)

cst.dt.AllVars.model.predictions <- predict(cst.dt.AllVars.model.cp0007, newdata = cst.testing.data[,-1], cp = 0.007)

cst.dt.AllVars.model.cp0007.ASE = mean((cst.testing.data[,1] - predict(cst.dt.AllVars.model.cp0007, cst.testing.data[,-1]))^2)
cst.dt.AllVars.model.cp0007.RMSE = sqrt(mean((cst.testing.data[,1] - predict(cst.dt.AllVars.model.cp0007, cst.testing.data[,-1]))^2))

cst.dt.AllVars.OvP.df.cp0007 <- data.frame(predicted = predict(cst.dt.AllVars.model.cp0007), observed= cst.training$Life.expectancy)

summary(cst.dt.AllVars.model.cp0007)


# Draw plot using ggplot2 package this time
```

```{r AIC decesion tree}

AIC(cst.dt.AllVars.OvP.df.cp0007$observed, cst.dt.AllVars.OvP.df.cp0007$predicted,nrow(cst.dt.AllVars.OvP.df.cp0007) ,19)
BIC(cst.dt.AllVars.OvP.df.cp0007$observed, cst.dt.AllVars.OvP.df.cp0007$predicted,nrow(cst.dt.AllVars.OvP.df.cp0007) ,19)
```

```{r DT model Expected Vs predicted}
ggplot(cst.dt.AllVars.OvP.df.cp0007,                                     
       aes(x = predicted,
           y = observed)) +
  geom_point() +
  geom_abline(intercept = 0,
              slope = 1,
              color = "red",
              size = 2)

```

```{r lasso with glmnet}

library(glmnet)



x = model.matrix(cst.training$Life.expectancy ~ . ,cst.training)[,-1]
y = cst.training$Life.expectancy

xtest<-model.matrix(cst.testing.data$Life.expectancy~.,cst.testing.data)[,-1]
ytest<-(cst.testing.data$Life.expectancy)

grid=10^seq(10,-2, length =100)
lasso.mod=glmnet(x,y,alpha=1, lambda =grid)

cv.out=cv.glmnet(x,y,alpha=1) #alpha=1 performs LASSO
plot(cv.out)
bestlambda<-cv.out$lambda.min  #Optimal penalty parameter.  You can make this call visually.
lasso.pred=predict (lasso.mod ,s=bestlambda ,newx=xtest)

testMSE_LASSO<-mean((ytest-lasso.pred)^2)
testMSE_LASSO


coef(lasso.mod,s=bestlambda)

```

```{r lasso with caret}
library(dplyr)
set.seed(1234)
x <- model.matrix(Life.expectancy~.,cst.training)[,-1]
y <- cst.training$Life.expectancy
xtest <- model.matrix(Life.expectancy~.,cst.testing.data)[,-1]
ytest <- cst.testing.data$Life.expectancy
grid=10^seq(10,-2, length =100)
lasso<-train(y= y,
                 x = x,
                 method = 'glmnet', 
                 tuneGrid = expand.grid(alpha = 1, lambda = grid)
               ) 

ridge <-train(y=y, x = x, method = 'glmnet',tuneGrid = expand.grid(alpha = 0, lambda = grid))

predictions_lasso <- lasso %>% predict(xtest)
predictions_ridge <- ridge %>% predict(xtest)

Lasso_RMSE = RMSE(predictions_lasso, ytest)
Lasso_RMSE


Ridge_RMSE = RMSE(predictions_ridge, ytest)
Ridge_RMSE

#Post resample - Lasso
lasso.cst.testing.data<-postResample(pred = predictions_lasso, obs = cst.testing.data$Life.expectancy)
lasso.cst.testing.data
ridge.cst.testing.data<-postResample(pred = predictions_ridge, obs = cst.testing.data$Life.expectancy)
ridge.cst.testing.data

coef(lasso$finalMode,lasso$finalModel$lambdaOpt)
varImp(lasso)
coef(ridge$finalModel,ridge$finalModel$lambdaOpt)
varImp(ridge)

ImpLasso <- varImp(lasso)
ImpRidge <- varImp(ridge)

plot(ImpLasso)
plot(ImpRidge)

```

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                         Run models Again of VarIMP
                         Filtered.Income.composition.of.resources
                         log.adj.Adult.Mortality
                         log.HIV.AIDS
                         Developed
                         log.under.five.deaths
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  filtered.Income.composition.of.resources, log.adj.Adult.Mortality, log.HIV.AIDS, Developed, log.under.five.deaths
```{r}
# function is not liking all these variables will use select to filter training set
cst.knn.5var.model <- knnreg(cst.training$Life.expectancy ~ filtered.Income.composition.of.resources, log.adj.Adult.Mortality, log.HIV.AIDS, Developed, log.under.five.deaths, data = cst.training, k=2)
```


```{r}

cs.5var.training.data <- subset(cst.training, select = c(Life.expectancy,filtered.Income.composition.of.resources, log.adj.Adult.Mortality, log.HIV.AIDS, Developed, log.under.five.deaths))
cs.5var.testing.data <- subset(cst.testing.data, select = c(Life.expectancy,filtered.Income.composition.of.resources, log.adj.Adult.Mortality, log.HIV.AIDS, Developed, log.under.five.deaths))

```


```{r}


train5var.knn = function(mr.rogers){
  knnreg(cs.5var.training.data$Life.expectancy ~ ., data = cs.5var.training.data, k=mr.rogers)
} 

test5var.knn =  function(mr.rogers){
  knnreg(cs.5var.testing.data$Life.expectancy ~ ., data = cs.5var.testing.data, k=mr.rogers)
} 

k.to.try <- seq(1,50, by = 1)
#create and store list knn predictions for k = 1 thru fiddy
cst.knn.5var.model.list.train = lapply(k.to.try, train5var.knn)
cst.knn.5var.model.list.test = lapply(k.to.try, test5var.knn)


#create and store list of predictions
cst.knn.5var.model.train.predictions <- lapply(cst.knn.5var.model.list.train, predict, cs.5var.training.data)
cst.knn.5var.model.test.predictions <- lapply(cst.knn.5var.model.list.test, predict, cs.5var.testing.data)
```

Model Metrics RMSE
```{r Root Mean Square Error}
#create function for RMSE


#create and store matrix of RMSE
cst.knn.5var.model.rmse.train <- sapply(cst.knn.5var.model.train.predictions, calc.rmse, actual = cs.5var.training.data$Life.expectancy)
cst.knn.5var.model.rmse.test <- sapply(cst.knn.5var.model.test.predictions, calc.rmse, actual = cs.5var.testing.data$Life.expectancy)

#RMSE plot of training vs predicted 
plot(k.to.try, cst.knn.5var.model.rmse.test, type = "b", col = "dodgerblue", pch = 20, 
     ylim = range(c(cst.knn.rmse.test, cst.knn.rmse.train)),
     xlab = "K Neighbors",
    ylab = "RMSE Specificy units (years?",
    main = "Test and Training RMSE vs K")
lines(k.to.try, cst.knn.5var.model.rmse.train, type = "b", col = "darkorange", pch = 23)
index<-which(cst.knn.5var.model.rmse.test ==min(cst.knn.rmse.test))
points(index,cst.knn.5var.model.ase.test[index],col="red",pch=10)
legend("topright", c("Train RMSE", "Test RMSE"), 
       col = c("darkorange", "dodgerblue"),
       lty = c(1), 
       pch = c(23, 20))
grid()
```

```{r}
cst.knn.5var.model.ase.train <- sapply(cst.knn.5var.model.train.predictions, calc.ase, actual = cs.5var.training.data$Life.expectancy)
cst.knn.5var.model.ase.test <- sapply(cst.knn.5var.model.test.predictions, calc.ase, actual = cs.5var.testing.data$Life.expectancy)

plot(k.to.try, cst.knn.5var.model.ase.test, type = "b", col = "dodgerblue", pch = 20, 
     ylim = range(c(cst.knn.ase.test, cst.knn.5var.model.rmse.train)),
     xlab = "K Neighbors",
    ylab = "ASE",
    main = "Test and Training ASE vs K")
lines(k.to.try, cst.knn.5var.model.ase.train, type = "b", col = "darkorange", pch = 23)
index<-which(cst.knn.5var.model.ase.test ==min(cst.knn.5var.model.ase.test))
points(index,cst.knn.5var.model.ase.test[index],col="red",pch=10)
legend("topright", c("Train ASE", "Test ASE"), 
       col = c("darkorange", "dodgerblue"),
       lty = c(1), 
       pch = c(23, 20))
grid()
```

```{r predicted vs expected}
knn.5var.k1 <- knnreg(x= cs.5var.training.data[,-1], y=cs.5var.training.data[,1], k=1)

#predict(knnt3, newdata = cs.5var.testing.data[,-1])

#data frame of predicted vs expected
cst.knn.5Vars.OvP.df.k1 <- data.frame(predicted = predict(knn.5var.k1, newdata = cs.5var.testing.data[-1]), observed= cs.5var.testing.data[,1])

knn.5var.k1.mse <- round(mean((cst.knn.5Vars.OvP.df.k1$predicted - cst.knn.5Vars.OvP.df.k1$observed)^2), digits = 3)
knn.5var.k1.rmse <- round(sqrt(mean((cst.knn.5Vars.OvP.df.k1$predicted - cst.knn.5Vars.OvP.df.k1$observed)^2)), digits = 3)



table(knn.5var.k1.mse, knn.5var.k1.rmse)

#they said it couldn't be done but I did it. plotted observed vs predicted with caret package. 
ggplot(cst.knn.5Vars.OvP.df.k1,                                     
       aes(x = predicted,
           y = observed)) +
  geom_point() +
  geom_abline(intercept = 0,
              slope = 1,
              color = "red",
              size = 2)

```

```{R compare knn model metrics}

cst.knn.5var.model.AIC <- AIC(cst.knn.5Vars.OvP.df.k1$observed, cst.knn.5Vars.OvP.df.k1$predicted, nrow(cst.knn.5Vars.OvP.df.k1), 5)

cst.knn.5var.model.BIC <- BIC(cst.knn.5Vars.OvP.df.k1$observed, cst.knn.5Vars.OvP.df.k1$predicted, nrow(cst.knn.5Vars.OvP.df.k1), 5)

table(cst.knn.AllVars.k38.AIC, cst.knn.AllVars.k38.BIC, cst.knn.AllVars.k1.AIC, cst.knn.AllVars.k1.BIC, cst.knn.5var.model.AIC, cst.knn.5var.model.BIC)
```